"""
The OpenAI backend. Uses large language models for black box optimization.
"""

import os
from ast import literal_eval

import openai

from bbopt.params import param_processor
from bbopt.backends.util import StandardBackend


# Constants:

# DEFAULT_ENGINE = "text-davinci-002"
DEFAULT_ENGINE = "text-curie-001"

DEFAULT_TEMP = 1.1
MAX_TEMP = 2

DEFAULT_MAX_RETRIES = 10

MAX_CONTEXT_ERR_PREFIX = "This model's maximum context length is "


# Utilities:

def get_prompt(params, data_points, losses) =
    """Get the OpenAI API prompt to use."""
    '''# black box function to be minimized
def f({func_params}) -> float:
    """
    parameters:
{docstring}

    returns:
        float: the loss
    """
    return black_box_function({names})

# known values (MUST stay within the bounds, SHOULD fully explore the bounds, SHOULD converge to minimum)
# bounds: f({domains})
{values}
assert f('''.format(
        func_params=", ".join(
            "{name}: {type}".format(
                name=name,
                type=(
                    "int" if func == "randrange"
                    else type(args[0][0]).__name__ if func == "choice" and all_equal(map(type, args[0]))
                    else "typing.Any" if func == "choice"
                    else "float"
                ),
            )
            for name, (func, args, _) in params.items()
        ),
        docstring="\n".join(
            "        {name}: in {func}({args})".format(
                name=name,
                func=func,
                args=", ".join(args |> map$(repr)),
            )
            for name, (func, args, _) in params.items()
        ),
        names=", ".join(params),
        domains=", ".join(
            "{func}({args})".format(
                func=func,
                args=", ".join(args |> map$(repr)),
            )
            for name, (func, args, _) in params.items()
        ),
        values="\n".join(
            "assert f({args}) == {loss}".format(
                args=", ".join(params |> map$(point[]) |> map$(repr)),
                loss=loss,
            )
            for point, loss in zip(data_points, losses)
        ),
    )


def get_completion_len(data_points) =
    """Get the maximum number of characters in a completion."""
    max(
        len(", ".join(point.values() |> map$(repr)))
        for point in data_points
    ) + 1


def to_python(completion, params):
    """Convert a completion to Python code as best as possible."""
    for repl, to in (
        ("\u2212", "-"),
        ("\u2018", "'"),
        ("\u2019", "'"),
        ("\u201c", '"'),
        ("\u201d", '"'),
    ) :: (
        (f"{name}=", "") for name in params
    ):
        completion = completion.replace(repl, to)
    return completion


# Backend:

class OpenAIBackend(StandardBackend):
    """OpenAI large language model BBopt backend."""
    backend_name = "openai"
    implemented_funcs = (
        "randrange",
        "uniform",
        "normalvariate",
        "choice",
    )

    max_prompt_len = float("inf")

    def setup_backend(self, params, engine=DEFAULT_ENGINE, temperature=DEFAULT_TEMP, max_retries=DEFAULT_MAX_RETRIES, api_key=None, debug=False):
        self.params = params

        self.engine = engine
        self.temp = temperature
        self.max_retries = max_retries
        openai.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.debug = debug

        self.data_points = []
        self.losses = []

    def tell_data(self, new_data, new_losses):
        self.data_points += new_data
        self.losses += new_losses

    def retry_get_values(self, temp=None):
        if not self.max_retries:
            raise RuntimeError("Maximum number of OpenAI API retries exceeded.")
        if self.debug:
            if temp is None:
                print(f"RETRYING with: {self.max_prompt_len=}")
            else:
                print(f"RETRYING with new temperature: {self.temp} -> {temp}")
        old_retries, self.max_retries = self.max_retries, self.max_retries - 1
        if temp is not None:
            old_temp, self.temp = self.temp, temp
        try:
            return self.get_next_values()
        finally:
            self.max_retries = old_retries
            if temp is not None:
                self.temp = old_temp

    def get_next_values(self):
        # generate prompt
        prompt = get_prompt(self.params, self.data_points, self.losses)
        while len(prompt) > self.max_prompt_len:
            self.data_points.pop(0)
            self.losses.pop(0)
            prompt = get_prompt(self.params, self.data_points, self.losses)
        if self.debug:
            print("\n== PROMPT ==\n" + prompt)

        # query api
        try:
            response = openai.Completion.create(
                engine=self.engine,
                prompt=prompt,
                temperature=self.temp,
                max_tokens=get_completion_len(self.data_points),
            )
        except openai.error.InvalidRequestError as api_err:
            if self.debug:
                print("== END ==")
            if not str(api_err).startswith(MAX_CONTEXT_ERR_PREFIX):
                raise
            if self.max_prompt_len == float("inf"):
                self.max_prompt_len = len(prompt.rsplit("\n")[0])
            else:
                self.max_prompt_len -= get_completion_len(self.data_points)
            if self.debug:
                print(f"ERROR: got max context length error")
            return self.retry_get_values()

        # parse response
        try:
            completion = response["choices"][0]["text"]
            if self.debug:
                print("== COMPLETION ==\n" + completion)
            valstr = to_python(completion.split(")", 1)[0].strip(), self.params)
            valvec = literal_eval("(" + valstr + ",)")
            assert len(valvec) == len(self.params), f"got {len(valvec)} values, expected {len(self.params)}"
            assert all(
                param_processor.in_support(name, val, func, *args, **kwargs)
                for val, (name, (func, args, kwargs)) in zip(
                    valvec,
                    self.params.items(),
                )
            ), "completion value(s) not in support"
        except BaseException as parse_err:
            if self.debug:
                print("== END ==")
            if self.debug:
                print(f"ERROR: {parse_err} for API response:\n{response}")
            return self.retry_get_values(temp=(self.temp + DEFAULT_TEMP) / 2)
        if self.debug:
            print("== END ==")

        # return values
        values = {name: val for name, val in zip(self.params, valvec)}
        if values in self.data_points:
            if self.debug:
                print("ERROR: OpenAI API generated duplicate value")
            return self.retry_get_values(temp=self.temp + (MAX_TEMP - self.temp) / 2)
        return values


# Registered names:

OpenAIBackend.register()
OpenAIBackend.register_alg("openai")
